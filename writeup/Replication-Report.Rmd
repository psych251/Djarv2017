---
title: "Replication of Study 2 in Djärv, Zehr, & Schwarz (2017, Proceedings of Sinn und Bedeutung 21)"
author: "Penny Pan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

Which property defines factive predicates has been the center of disagreement in the literature of presupposition and factivity (Degen & Tonhauser, 2022). Some scholars consider a clause-embedding predicate to be factive if its complement clause is presupposed (Kiparsky & Kiparsky, 1970). In this case, the projection diagnosis is commonly used to test the presupposed content of the factive predicates--the truth value of the embedded content projects under entailment-canceling operators, such as negation and polar question. On the other hand, some recent scholars define factivity as combination of presupposition and entailment (Abrusán, 2011). From this perspective, not only the speaker is considered to be committed to the embedded content, the listener can also infer the embedded content from the affirmative sentence. 

Djärv et al. (2017) shows cognitive and emotive factives differ in the entailment of the presupposed content. Specifically, cognitive factives, such as *realize* and *be aware*, entail both the presupposed content in the embedded clause and the attitude or behavior indicated by the predicate. On the other hand, emotive factives, such as *appreciate* and *be happy*, only entail the attitude/behavior. Given a yes/no-continuation acceptability judgement task, the participant rated the *yes, although...* continuation to be more natural for a question with an emotive factive than for a question that contains a cognitive factive.

(Examples from the original paper)

John was happy that his parents are coming to town, although it turned out he was in fact mistaken/although it turned out that they had to cancel. (Both felicitous).

?? John discovered that his parents are coming to town, although it turned out he was in fact mistaken/although it turned out that they had to cancel. (Not acceptable as they lead to contradiction).

Here are the links to [the experiment demo](https://farm.pcibex.net/r/xlqNje/), [the project repository](https://github.com/psych251/Djarv2017), and [the original paper](https://semanticsarchive.net/Archive/DRjNjViN/DjaervZehrSchwarz.pdf)


## Methods

### Power Analysis

<!-- Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size. Considerations of feasibility for selecting planned sample size. -->
```{r message=FALSE, warning=FALSE}
# power analysis: simulation to estimate the sample size
####Load Relevant Libraries and Functions
library("simr")
library("bootstrap")
library("devtools")
library("mixedpower")
library("knitr") # for knitting things
library("readr")
library("lme4")
library("emmeans")
library("brms")
library("janitor")     # for cleaning column names
library("tidybayes")
library("dplyr")
library("tidyverse") # for all things tidyverse

subject <- factor(1:62)
items <- paste0("item", c(1:24))
predicate <- c("emotive", "cognitive")
answer <- c("yes", "no")

subj_full <- rep(subject, 24)
items_full <- rep(items, 62)
answer_full <- rep(rep(answer, each=12),62)
predicate_full <- rep(rep(rep(predicate, each=6),2),62)


covars <- data.frame(id = subj_full, 
                     answer_type = answer_full, 
                     predicate_type = predicate_full,
                     items = items_full)

# sanity check to make sure the numbers are correct
check_covars <- covars |>
  group_by(id, predicate_type, answer_type) |>
  summarise(count  = n())
check_covars

fixed <- c(2.53, 1.626, 0.042, 0.8605) # estimate the slopes and intercepts for the fixed effects (based on the reported beta score of the interaction effect)
# rand <- matrix(c(2, 1, 0.5, 0.5), 2) # hypothetical estimates of the slopes and intercepts for the random effects. not sure where to obtain these values
rand <- 0.05
res <- 1.5

# create an lmer (prediction not quite right)
model <- makeLmer(rating ~ answer_type * predicate_type + (1 | id),
                  fixef = fixed,
                  VarCorr = rand,
                  sigma = res,
                  data = covars)


# mixedpower(model, data=covars, fixed_effects = c("answer_type", "predicate_type"),
#            simvar = "id", steps=c(55,60,65,70), critical_value=4.083, n_sim=100)

# simulate the lmer
sim <- powerSim(model, nsim=400, test = fcompare(y~answer_type + predicate_type +  (1 | id)))
sim

# extend not really working
# model_ext <- extend(model, along="id", n=24)
# p_curve_interaction <- powerCurve(model_ext,
#                             test = fcompare(y~answer_type + predicate_type +  (1 | id)),
#                             along ="id",
#                             nsim=200,
#                             breaks= c(60,62,64,66,68),
#                             alpha=.05)
# print(p_curve_interaction)
```
I used the reported coefficient $\beta$, standard error, and the t-score for the interaction effect to estimate the coefficients for the fixed effects. Hypothetical estimations of the slopes and intercepts for the random effects were used. Based on the sample size of 62 is able to achieve around 80% power.


### Planned Sample

<!--Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any. -->
I plan to recruit 62 participants, which is the same number of participants in the original study. All participants are native English speakers.

### Materials

Each stimulus in the experiment consisted of one question and one response.^[Thanks to authors of the original paper for providing the original experimental setup on PCIbex.] The response was either in the form *yes, although...*, which first provides an affirmation and then denies the content of the embedded content, or in the form *no, because...*, which directly provides a negative response, as shown in (1) and (2), respectively. 


__(1) Yes-response block__

Q: \{Is Maria aware/happy\} that [Mike is moving back to Chicago]?

A: Yes, although he isn't.

__(2) No-response block__

Q: \{Is Maria aware/Is Maria happy\} that [Mike is moving back to Chicago]?

A: No, because he isn't.


In the original study, 24 items were used as the embedded content, and each item was paired with both types of predicates (cognitive and emotive) and both types of responses (*yes, although...* or *no, because...*), resulted in four variations of each item. In addition, the authors considered two emotive predicates, *appreciate* and *be happy*, and two cognitive predicates, *realize* and *be aware*, so that for each type of predicate, one was verbal and the other was adjective, in order to control for the syntactic category. The two types of continuations were presented in two separate blocks (*yes-response block* and *no-response block*), and the order of the blocks was randomized across participants.

In addition, 48 fillers were included in the study. In the original report, Djärv et al. report that "[...Fillers] were designed with two purposes in mind: first, to provide a floor and a ceiling baseline for the yes- and no-responses; and second, to counterbalance the number of good and bad yes- and no-responses. Half of the fillers were therefore constructed using a non-factive matrix predicate (think), where the no- answers would be infelicitous, and the yes-answers would be fully acceptable, as in (16, *relabeled as Example 3 below*). The other half of the fillers involved a question with two conjuncts, as in (17, *relabeled as Example 4 below*). Here, it would be the yes-answers that were infelicitous, while no would be an acceptable response." 

__(3) Think__

Q. Does Sue think that Bill’s parents are going to the wedding?

A1. \#No, because they are. ("Bad control")

A2. Yes, although they aren’t. ("Good control")


__(4) Conjunction__

Q. Is John going to Paris and Rome this summer?

A1. No, he's not. ("Good control")

A2. \#Yes, although he isn't going to Rome. ("Bad control")


In sum, sentences with *think* and conjunction *A and B* that have canonically acceptable and unacceptable continuations were used as fillers, and a closer inspection of the stimuli set reveals that each filler sentence was paired with four possible responses, two for each type of continuations such that two continuations were functioned as good controls and two as bad controls. In other words, *think* with no continuation (A2 in Example 5) and conjunct structure with yes continuation (A4 in Example 6) were also used as good controls, and likewise, *think* with yes continuation (A4 in Example 5) and conjunct with no continuation (A2 in Example 6) were also used as bad controls, resulting in fully crossed between the control type and the continuation type. The full stimuli set is included in the supplement. 

__(5) Think__

Q. Does Ryan think that Anna is going to the wedding reception?

A1. \#No, because she is. ("Bad control")

A2. No, because she isn't. ("Good control")

A3. Yes, although she isn't. ("Good control")

A4. \#Yes, although he thinks that only Lisa is. ("Bad control")


__(6) Conjunction__

Q. Is Mike going to Paris and Rome this summer?

A1. No, he's not. ("Good control")

A2. \#No, because he is. ("Bad control")

A3. \#Yes, although he isn't going to Paris. ("Bad control")

A4. Yes, although he's only spending a day in each city. ("Good control")


All items were divided into two blocks based on the answer type. Both the order of the blocks and the order of sentence pairs within each block were randomized.


### Procedure	

Each participant saw 72 pairs of question and answer, including 24 critical pairs and 48 filler pairs, and was asked to rate the naturalness of each answer based on a Likert scale, with 1 being "completely unnatural" to 7 being "completely natural." 

### Analysis Plan

<!--Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible. -->

The authors used linear mixed effects regression models to analyze the ratings of the critical pairs. Specifically, they included the predicate type and the answer type as fixed effects. Then, the considered three models, one with the additional main effect of syntactic category, one with the effect of block order, and one simple model with neither of the two. The complex model with all four factors did not converge, and thus was not used. They "tested for the maximally complex models, including all possible interactions of predictors and all random slopes for participants and items as random effects, and (their) different baselines exhausted the logical space of effects and interactions" (p.13).

According to the results reported in the original paper, adding the two additional factors of block order and syntactic category did not significantly change the results. This is as predicted, since neither the block order nor the syntactic category of the predicate should affect the entailment of the embedded content. 

Thus, the simple linear mixed-effects regression model is the key statistical model, which includes the main effects of answer type (reference level=yes), predicate type (reference level=cognitive) and their interaction and the maximal random effects structure: rating ~ predicate_type * answer_type + (predicate_type * answer_type  | participant) + (predicate_type * answer_type | item). 
The key item of interest is the interaction term, with the prediction: emotive factives will have higher ratings than cognitive factives for yes-responses.

In addition, I also plan to use Bayesian models to analyze the data as an exploratory analysis. Last, since the non-factive verb *think* is used as a control item, I would also like to compare the difference in the entailment of the embedded content between factive and non-factive verbs. Given the hypothesis that non-factives do not entail the truth of the complement clause, the rating of `Think Good` (Example 3 A2) with the yes-continuation should be higher than both the cognitive and emotive factives with yes-continuation. Moreover, non-factive *think* will not have significantly different ratings for the yes and no-responses, since the embedded content is not entailed, and either affirming or denying the matrix clause leaves the truth of embedded content undetermined.
<!-- You can also pre-specify additional analyses you plan to do.-->

### Differences from Original Study

<!--Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect. -->
The replication aims to adhere to the original study as close as possible, and the only two possible sources of differences are 1) the sample and 2) the experimental setting. The original study recruited college students who were native English speakers at the University of Pennsylvania. However, the current study will use the Prolific platform to recruit any native English speakers in the US. In addition, the original study was an in-lab experiment, yet the current experiment will be fully online. Nonetheless, the differences in the experimental setup should not affect the result.

<!-- ### Methods Addendum (Post Data Collection) -->
<!-- You can comment this section out prior to final report with data collection. -->

#### Actual Sample
Sixty two native English speakers were recruited through Prolific. Data from two participants were not successfully recorded. The remaining 60 participants all self-reported as native English speakers, and all data were included in the analysis. The experiment took 15 minutes to complete, and each participant was paid $2.

#### Differences from pre-data collection methods plan
None.

## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r eval=FALSE, message=FALSE, warning=FALSE, include=FALSE}
###Data Preparation

####Import data
# ignore the first 17 lines as they are explanations of the content in each column
# df.data <- read.csv("../raw_data/pilot/results.csv", header=FALSE, skip=17)
df.data <- read.csv("../raw_data/main/results.csv", header=FALSE, skip=17)
# add column names, where
# ip -> participant, 
# latin_square -> stimulus_type (either control or filler)
colnames(df.data) <- c("reception_time", "ip", "controller", "order", "inner_element", "label", "stimulus_type", "legend","answer","correct","rt")


#### Data exclusion / filtering
# exclude participants who did not self-report as native English speakers (added question after pilot)
language <- subset(df.data, legend == "native_language" & str_detect(answer, regex("english", ignore_case = TRUE)))
native_speaker <- language$ip
df.data.tidy <- subset(df.data, ip %in% native_speaker)

df.data.tidy <- df.data.tidy |>
  group_by(ip) |>
  mutate(participant = cur_group_id()) |>
  ungroup() |>
  relocate(participant) |>
  select("participant", "legend", "label", "stimulus_type", "answer", "rt") |>
  filter(label!="practice") |>
  drop_na(rt) |> # drop the end of experiment questions 
  group_by(participant) |> # group by participant
  mutate(total_rt = sum(rt)) |>
  ungroup() |>
  select("participant", "legend", "stimulus_type", "answer", "label", "total_rt")

# save the anonymized for later use
# write.csv(df.data.tidy, "../data/pilot/cleaned_data.csv" , row.names = FALSE) 
write.csv(df.data.tidy, "../data/main/cleaned_data.csv" , row.names = FALSE)
```

```{r message=FALSE, warning=FALSE}
#### Prepare data for analysis - create columns etc.
# read the cleaned-up data file
# df.data.tidy <- read.csv("../data/pilot/cleaned_data.csv", header=TRUE)
df.data.tidy <- read.csv("../data/main/cleaned_data.csv", header=TRUE)
# extract the item name 
df.data.clean <- df.data.tidy |>
  mutate(item = gsub(".*<b>Item</b>_(.*)\\+<b>Group.*", "\\1", legend),
         Question = gsub(".*<b>Question</b>_(.*)\\+<b>Answer.*", "\\1", legend),
         Continuation = gsub(".*<b>Answer</b>_(.*)$", "\\1", legend),
         Continuation = gsub("%2C", ",", Continuation)) |>
  group_by(participant) |>
  distinct(item, .keep_all = TRUE) |> # each item has three entries (corresponds to three key-press events, two for space to advance, one for the actually numerical rating)
  ungroup() |>
  select(-legend)

# separate by different conditions
df.data.clean <- df.data.clean |>
  mutate(label_new = gsub('([[:upper:]])', ' \\1', label)) |>
  mutate(exp = str_split_fixed(label_new, ' ', 5)[,2],
         types = str_split_fixed(label_new, ' ', 5)[,3],
         syntactic_category = str_split_fixed(label_new, ' ', 5)[,4],
         answer_type = str_split_fixed(label_new, ' ', 5)[,5],
         predicate_type = ifelse(syntactic_category %in% c("Good", "Bad"), syntactic_category, types),
         all_types = ifelse(syntactic_category %in% c("Good", "Bad"), paste(types, syntactic_category), types)) |>
    mutate(predicate_type = recode(predicate_type,
                          Bad = "Bad control",
                          Cog = "Cognitive",
                          Emo = "Emotive",
                          Good = "Good control"),
           all_types = recode(all_types,
                              Conj_Good = "Good_Conjunct",
                              Conj_Bad = "Bad_Conjunct",
                              Think_Good = "Good_Think",
                              Think_Bad = "Bad_Think",
                              Cog = "Cognitive",
                              Emo = "Emotive")) |>
  select(-label_new)

# change the answers to numerical values
df.data.clean <- df.data.clean |>
  mutate(rating = case_when(
    answer == "CompUnnatural" ~ 1,
    answer == "Unnatural" ~ 2,
    answer == "NotNatural" ~ 3,
    answer == "Average" ~ 4,
    answer == "Natural" ~ 5,
    answer == "QuiteNatural" ~ 6,
    answer == "CompNatural" ~ 7
  )) |> 
  select(-answer)

# get the stimuli and check the number of each item
df.items <- df.data.clean |>
  select(item, predicate_type, answer_type, syntactic_category, all_types, label, Question, Continuation) |>
  relocate(all_types, .after = item) |>
  group_by(item, answer_type,  Question, Continuation, all_types) |>
  summarize(num = n())
# write.csv(df.items, file="stimuli.csv")
```

```{r message=FALSE, warning=FALSE}
# plot the result
# set the theme
theme_set(theme_bw())
# color-blind-friendly palette -> currently not used to match the plot in the original study
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7") 

## helper functions for the graph
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}
ci.low <- function(x,na.rm=T) {
  mean(x,na.rm=na.rm) - quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)}
ci.high <- function(x,na.rm=T) {
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) - mean(x,na.rm=na.rm)}

data_summary <- df.data.clean |>
  group_by(predicate_type, answer_type) |>
  summarise(mean_rating = mean(rating),
            ci_low = ci.low(rating),
            ci_high = ci.high(rating)) |>
  ungroup() |>
  mutate(YMin = mean_rating - ci_low,
         YMax = mean_rating + ci_high,
         predicate_type = fct_relevel(predicate_type, "Bad control", "Cognitive", "Emotive", "Good control"),
         answer_type = fct_relevel(answer_type, "Yes", "No"))

summary_plot <- ggplot(data_summary,
       aes(x = predicate_type,
           y = mean_rating,
           fill = predicate_type)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=YMin,
                    ymax=YMax),
                width=.2) +
  facet_grid(. ~ answer_type) +
  labs(x = "Type",
       y = "Score (1=Compl. unnatural, 7=Compl. natural)",
       fill = "Predicate") +
  scale_fill_discrete(guide="none") +
  scale_x_discrete(labels = c("Bad control", "Cognitive", "Emotive", "Good control"))
# summary_plot
# ggsave(summary_plot, file="graphs/replication_results.png", width=6, height=4)
```


```{r main_plot, echo=FALSE,out.width="49%", out.height="20%",fig.cap="Meaning ratings by answer type and predicate type. Plot on the left is from the original study and on the right is the plot of the current study. The error bars represent the 95% bootstrapped confidence intervals.",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("graphs/original_results.png","graphs/replication_results.png"))
``` 
Overall, the results in the graphs look similar.^[In the figure of the original study, the score might be in the range from 1 to 7, with 1 being completely unnatural and 7 being completely natural, instead of 0 to 6 as shown in the title of the y-axis. This is because the average no-rating of the *Good controls* is slightly above 6, which would exceed the maximum if it was on a 0-6 scale.]

### Confirmatory analysis

As specified in the analysis plan, a linear mixed-effect model was conducted to predict the rating from the main effect of predicate types and answer types and the maximal random effect structure.

```{r message=FALSE}
df.data.clean$predicate_type <- as.factor(df.data.clean$predicate_type)
df.data.clean$predicate_type <- relevel(df.data.clean$predicate_type, ref = "Cognitive")
df.data.clean$answer_type <- as.factor(df.data.clean$answer_type)
df.data.clean$answer_type <- relevel(df.data.clean$answer_type, ref = "Yes")

# failed to converge with (predicate_type * answer_type | item), so the by-item random slope is excluded
simple_model = lmer(rating ~ predicate_type * answer_type + (predicate_type * answer_type | participant) + (predicate_type * answer_type | item),
             data = df.data.clean |>
               filter(stimulus_type == "C"))

summary(simple_model)

contrasts <- list(emotive_vs_cognitive = c(-1, 1),
                 yes_vs_no = c(1, -1))

simple_emmeans <- simple_model |>
  emmeans(specs = pairwise ~ predicate_type + answer_type,
          contr = contrasts)

simple_emmeans

sigma(simple_model)
eff_size(simple_emmeans, sigma = sigma(simple_model), edf=35.30)

```

### Exploratory analyses

#### Bayesian analysis
```{r}
# Using the default priors in the brms package
# Bulk Effective Samples Size (ESS) and Tail Effective Samples (ESS) are too low
brm_model = brm(rating ~ predicate_type * answer_type + (predicate_type * answer_type | participant) + (predicate_type * answer_type | item),
                data = df.data.clean |>
                  filter(stimulus_type == "C"),
                seed = 1,
                family = gaussian(),
                file = "../cache/brm1")

brm_model |>
  summary()

brm_model |>
  spread_draws(b_Intercept) |>
  clean_names() |>
  mutate(chain = as.factor(chain)) |>
  ggplot(aes(x = iteration,
             y = b_intercept,
             group = chain,
             color = chain)) + 
  geom_line() + 
  scale_color_brewer(type = "seq",
                     direction = -1)
```

#### Factives v.s. non-factives

```{r message=FALSE, warning=FALSE}
# exploratory: compare think (non-factive) and the two types of factives, not collapsing over conditions 
data_summary_all <- df.data.clean |>
  group_by(all_types, answer_type) |>
  summarise(mean_rating = mean(rating),
            ci_low = ci.low(rating),
            ci_high = ci.high(rating)) |>
  ungroup() |>
  mutate(YMin = mean_rating - ci_low,
         YMax = mean_rating + ci_high,
         answer_type = fct_relevel(answer_type, "Yes", "No"))

summary_think_plot <- ggplot(data_summary_all |>
                                 filter(!all_types %in% c("Conj Bad", "Conj Good")) |>
                               mutate(all_types = fct_relevel(all_types, "Think Bad", "Cognitive", "Emotive", "Think Good")),
       aes(x = all_types,
           y = mean_rating,
           fill = all_types)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=YMin,
                    ymax=YMax),
                width=.2) +
  facet_grid(. ~ answer_type) +
  labs(x = "Type",
       y = "Score (1=Compl. unnatural, 7=Compl. natural)",
       fill = "Predicate") +
    scale_fill_discrete(guide="none")
# summary_think_plot
# ggsave(summary_think_plot, file="graphs/replication_think.png", width=7, height=4)
``` 

```{r exploratory_plot, fig.align="center", echo=FALSE,out.width="80%", out.height="20%",fig.cap="Acceptability ratings of cognitives, emotives, and non-factive control (think). The error bars represent the 95% bootstrapped confidence intervals.",fig.show='hold',fig.align='center'}
knitr::include_graphics("graphs/replication_think.png")
``` 

```{r message=FALSE}
df.data.clean$all_types <- as.factor(df.data.clean$all_types)
df.data.clean$all_types <- relevel(df.data.clean$all_types, ref = "Think Good")
df.data.clean$answer_type <- relevel(df.data.clean$answer_type, ref = "Yes")

exploratory_model = lmer(rating ~ all_types * answer_type + (all_types * answer_type | participant) + (all_types * answer_type | item),
             data = df.data.clean |>
                filter(all_types %in% c("Cognitive", "Emotive", "Think Good")))

summary(exploratory_model)
joint_tests(exploratory_model)

exploratory_contrasts <- list(factive_vs_non = c(-0.5, -0.5, 1),
                 yes_vs_no = c(1, -1))

exploratory_emmeans <- exploratory_model |>
  emmeans(specs = pairwise ~ all_types + answer_type,
          contr = exploratory_contrasts)

exploratory_emmeans

```

## Discussion

### Summary of Replication Attempt

<!-- Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.   -->

A linear mixed effects model was used to predict acceptability rating from a fixed effect of predicate type (reference level: `Cognitive`) and answer type (reference level: `Yes`) with the maximal effects structure with random by-participant and by-item intercepts and slopes. There was a significant interaction effect between predicate type and answer type (t = -3.637, $\beta$ = -0.813, SE = 0.223). In addition, emotive predicates had significantly higher Yes-ratings than cognitive predicates (t = 3.292, $\beta$ = 0.556, SE = 0.169). These results are in line with the findings in the original study. In terms of the No-ratings, emotive predicates did not significantly differ from the emotive predicates (t=-0.256, $\beta$ = 0.168, SE = -1.523).

All results were similar to results in the original study (interaction: t = 4.083 $\beta$ = 0.8601, SE = 0.2108, yes-ratings: t = 4.954, $\beta$ = 0.76, SE = 0.1534, no-ratings: t = 0.625, $\beta$ = 0.1005, SE = 0.1607).

### Commentary

<!-- Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long. -->

In sum, the results are in line with the original study, suggesting that the experiment is replicable and the entailment relationship of the factive predicates are potentially generalizable. The exploratory analysis using Bayesian mixed-effects model with the default priors using the brms package in R reveals similar results ($\beta$=0.56, SE=0.17, 95\% CI=[0.2,0.89]). Furthermore, the results in the exploratory analysis shows that the non-factive predicate *think* differs from both types of factive predicates with the yes-responses, but unlike emotive factives, *think* has high ratings with both yes and no continuations. A linear mixed-effect model was used to predict the effect of predicate type (cognitive factives, emotive factives, and non-factive *think*, reference level: Think) and answer type (reference level: yes) on rating. The yes-ratings of the non-factive *think* was significantly higher than the yes-ratings of the emotive factives (t=-12.018, $\beta$=-3.183, SE=0.265) and cognitive factives (t=-11.507, $\beta$=-2.643, SE=0.230). Moreover, for *think*, there was no significant difference between ratings in the yes-response and no-response (t=1.718, $\beta$=0.447, SE=0.260).

Overall, I was able to replicate the original results, such that the emotive and cognitive factives differ in the entailment of the embedded content. The similar paradigm can be used to further investigate the difference in the entailment relationships between different types of factive and non-factive predicates, in order to understand the defining properties of factivity.
