---
title: "Replication of Study 2 by Djärv, Zehr, & Schwarz (2017, Proceedings of Sinn und Bedeutung 21)"
author: "Penny Pan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

Djärv et al. (2017) shows cognitive and emotive factives differ in the entailment of the presupposed content. Specifically, cognitive factives, such as *realize* and *be aware*, entail both the presupposed content in the embedded clause and the attitude or behavior indicated by the predicate. On the other hand, emotive factives, such as *appreciate* and *be happy*, only entail the attitude/behavior. Given a yes/no-continuation acceptability judgement task, the participant rated the *yes, although...* continuation to be more natural for a question with an emotive factive than for a question that contains a cognitive factive.

(Examples from the original paper)

John was happy that his parents are coming to town, although it turned out he was in fact mistaken/although it turned out that they had to cancel. (Both felicitous).

?? John discovered that his parents are coming to town, although it turned out he was in fact mistaken/although it turned out that they had to cancel. (Not acceptable as they lead to contradiction).

Here are the links to [the experiment demo](https://farm.pcibex.net/r/xlqNje/), [the project repository](https://github.com/psych251/Djarv2017), and [the original paper](https://semanticsarchive.net/Archive/DRjNjViN/DjaervZehrSchwarz.pdf)


## Methods

### Power Analysis

<!-- Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size. Considerations of feasibility for selecting planned sample size. -->
```{r message=FALSE, warning=FALSE}
# power analysis: simulation to estimate the sample size
####Load Relevant Libraries and Functions
library("simr")
library("knitr") # for knitting things
library("readr")
library("lme4")
library("emmeans")
library("dplyr")
library("tidyverse") # for all things tidyverse

subject <- factor(1:62)
items <- paste0("item", c(1:24))
predicate <- c("emotive", "cognitive")
answer <- c("yes", "no")

subj_full <- rep(subject, 24)
items_full <- rep(items, 62)
answer_full <- rep(rep(answer, each=12),62)
predicate_full <- rep(rep(rep(predicate, each=6),2),62)

covars <- data.frame(id = subj_full, 
                     answer_type = answer_full, 
                     predicate_type = predicate_full,
                     items = items_full)

# sanity check to make sure the numbers are correct
check_covars <- covars |>
  group_by(id, predicate_type, answer_type) |>
  summarise(count  = n())
check_covars

fixed <- c(2.53, 1.626, 0.042, 0.8605) # estimate the slopes and intercepts for the fixed effects (based on the reported beta score of the interaction effect)
rand <- matrix(c(2, 1, 0.5, 0.5), 2) # hypothetical estimates of the slopes and intercepts for the random effects. not sure where to obtain these values
res <- -0.05

# create an lmer (prediction not quite right)
model <- makeLmer(rating ~ answer_type * predicate_type + (answer_type | id),
                  fixef = fixed,
                  VarCorr = rand,
                  sigma = res,
                  data = covars)

# simulate the lmer
# sim <- powerSim(model, nsim=100, test = fcompare(y~answer_type + predicate_type))
# sim

# not really working
# p_curve_interaction <- powerCurve(model,
#                             test = fcompare(y~answer_type + predicate_type),
#                             along ="id",
#                             breaks = c(60, 70, 80, 90),
#                             nsim=100,
#                             alpha=.05)
# 
# plot(p_curve_interaction)
```
I used the reported coefficient $\beta$, standard error, and the t-score for the interaction effect to estimate the coefficients for the fixed effects. Hypothetical estimations of the slopes and intercepts for the random effects were used.


### Planned Sample

<!--Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any. -->
Planned sample size is 62, which is the same as the sample size in the original study. All participants are native English speakers.

### Materials

Each stimulus in the experiment consisted of one question and one response. The response was either in the form *yes, although...*, which first provides an affirmation and then denies the content of the embedded content, or in the form *no, because...*, which directly provides a negative response, as shown in (1) and (2), respectively. 


__(1) Yes-response block__

Q: \{Is Maria aware/happy\} that [Mike is moving back to Chicago]?

A: Yes, although he isn't.

__(2) No-response block__

Q: \{Is Maria aware/Is Maria happy\} that [Mike is moving back to Chicago]?

A: No, because he isn't.


In the original study, 24 items were used as the embedded content, and each item was paired with both types of predicates (cognitive and emotive) and both types of responses (*yes, although...* or *no, because...*), resulted in four variations of each item. In addition, the authors considered two emotive predicates, *appreciate* and *be happy*, and two cognitive predicates, *realize* and *be aware*, so that for each type of predicate, one is verbal and the other is adjective, in order to control for the syntactic category.

In addition, 48 filler items were also included to elicit the baseline rating for the yes/no responses. Non-factive predicates, such as *think*, were used the baseline for *yes*, where yes-continuation would acceptable but not the no-continuation (Example 3). On the other hand, questions with two conjunctions were used as baseline for *no*, where the only felicitous response would be the no-continuation (Example 4). 

All items were divided into two blocks based on the answer type. Both the order of the blocks and the order of sentence pairs within each block were randomized.

__(3) *yes*-response baseline/ceiling__

Q. Does Sue think that Bill’s parents are going to the wedding?

A1. \#No, because they are. (infelicitous, "Bad control")

A2. Yes, although they aren’t. (acceptable, "Good control")


__(4) *no*-response baseline/ceiling__

Q. Is John going to Paris and Rome this summer?

A1. No, he's not. (acceptable, "Good control")

A2. \#Yes, although he isn't going to Rome. (infelicitous, "Bad control")

### Procedure	

Each participant saw 72 pairs of question and answer, including 24 critical pairs and 48 filler pairs, and was asked to rate the naturalness of each answer based on a Likert scale, with 1 being "completely unnatural" to 7 being "completely natural." 

### Analysis Plan

<!--Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible. -->

The authors used linear mixed effects regression models to analyze the ratings of the critical pairs. Specifically, they included the predicate type and the answer type as fixed effects. Then, the considered three models, one with the additional main effect of syntactic category, one with the effect of block order, and one simple model with neither of the two. The complex model with all four factors did not converge, and thus was not used. They "tested for the maximally complex models, including all possible interactions of predictors and all random slopes for participants and items as random effects, and (their) different baselines exhausted the logical space of effects and interactions" (p.13).

According to the results reported in the original paper, adding the two additional factors of block order and syntactic category did not significantly change the results. This is as predicted, since neither the block order nor the syntactic category of the predicate should affect the entailment of the embedded content. 
Thus, the simple linear mixed-effects regression model is the key statistical model, which includes the main effects of answer type (reference level=yes), predicate type (reference level=cognitive) and their interaction and the maximal random effects structure: rating ~ predicate_type * answer_type + (predicate_type * answer_type  | participant) + (predicate_type * answer_type | item). 
The key item of interest is the interaction term, with the prediction: emotive factives will have higher ratings than cognitive factives for yes-responses.


In addition, I also plan to use Bayesian models to analyze the data. 
<!-- You can also pre-specify additional analyses you plan to do.-->

### Differences from Original Study

<!--Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect. -->

<!-- ### Methods Addendum (Post Data Collection) -->

<!-- You can comment this section out prior to final report with data collection. -->

#### Actual Sample
Sixty native English speakers through Prolific. All participants self-reported as native English speakers, and all data were included in the analysis.

#### Differences from pre-data collection methods plan
None.

## Results

### Data preparation

Data preparation following the analysis plan.
	
```{r message=FALSE, warning=FALSE}
###Data Preparation

####Import data
# ignore the first 17 lines as they are explanations of the content in each column
# df.data <- read.csv("../data/pilot/results.csv", header=FALSE, skip=17)
df.data <- read.csv("../data/main/results.csv", header=FALSE, skip=17)
# add column names
# reception_time -> participant, latin_square -> stimulus_type (either control or filler)
colnames(df.data) <- c("reception_time", "ip", "controller", "order", "inner_element", "label", "stimulus_type", "legend","answer","correct","rt")


#### Data exclusion / filtering

# exclude participants who did not self-report as native English speakers
language <- subset(df.data, legend == "native_language" & str_detect(answer, regex("english", ignore_case = TRUE)))
native_speaker <- language$ip
df.data.tidy <- subset(df.data, ip %in% native_speaker)

df.data.tidy <- df.data.tidy |>
  group_by(ip) |>
  mutate(participant = cur_group_id()) |>
  ungroup() |>
  relocate(participant) |>
  select("participant", "legend", "label", "stimulus_type", "answer", "rt") |>
  filter(label!="practice") |>
  drop_na(rt) |> # drop the end of experiment questions 
  group_by(participant) |> # group by participant
  mutate(total_rt = sum(rt)) |>
  ungroup() |>
  select("participant", "legend", "stimulus_type", "answer", "label", "total_rt")


#### Prepare data for analysis - create columns etc.
# extract the item name 
df.data.clean <- df.data.tidy |>
  mutate(item = gsub(".*<b>Item</b>_(.*)\\+<b>Group.*", "\\1", legend),
         Question = gsub(".*<b>Question</b>_(.*)\\+<b>Answer.*", "\\1", legend),
         Continuation = gsub(".*<b>Answer</b>_(.*)$", "\\1", legend),
         Continuation = gsub("%2C", ",", Continuation)) |>
  select(-legend)

# separate by different conditions
df.data.clean <- df.data.clean |>
  mutate(label_new = gsub('([[:upper:]])', ' \\1', label)) |>
  mutate(exp = str_split_fixed(label_new, ' ', 5)[,2],
         types = str_split_fixed(label_new, ' ', 5)[,3],
         syntactic_category = str_split_fixed(label_new, ' ', 5)[,4],
         answer_type = str_split_fixed(label_new, ' ', 5)[,5],
         predicate_type = ifelse(syntactic_category %in% c("Good", "Bad"), syntactic_category, types),
         all_types = ifelse(syntactic_category %in% c("Good", "Bad"), paste(types, syntactic_category), types)) |>
    mutate(predicate_type = recode(predicate_type,
                          Bad = "Bad control",
                          Cog = "Cognitive",
                          Emo = "Emotive",
                          Good = "Good control"),
           all_types = recode(all_types,
                              Conj_Good = "Good_Conjunct",
                              Conj_Bad = "Bad_Conjunct",
                              Think_Good = "Good_Think",
                              Think_Bad = "Bad_Think",
                              Cog = "Cognitive",
                              Emo = "Emotive")) |>
  select(-label_new)

# change the answers to numerical values
df.data.clean <- df.data.clean |>
  mutate(rating = case_when(
    answer == "CompUnnatural" ~ 1,
    answer == "Unnatural" ~ 2,
    answer == "NotNatural" ~ 3,
    answer == "Average" ~ 4,
    answer == "Natural" ~ 5,
    answer == "QuiteNatural" ~ 6,
    answer == "CompNatural" ~ 7
  )) |> 
  select(-answer)

# check the number of each item
df.data.clean |>
  filter(stimulus_type == "C") |>
  group_by(item,predicate_type, answer_type) |>
  summarize(num = n())

# get the stimuli
df.items <- df.data.clean |>
  select(predicate_type, answer_type, syntactic_category, all_types, label, Question, Continuation)
df.unique.items <- distinct(df.items, label, .keep_all = TRUE)
items_summary <- df.unique.items |>
  group_by(all_types) |>
  summarise(count = n())
```

```{r message=FALSE, warning=FALSE}
# plot the result
# set the theme
theme_set(theme_bw())
# color-blind-friendly palette
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7") 

## helper functions for the graph
library(bootstrap)
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}
ci.low <- function(x,na.rm=T) {
  mean(x,na.rm=na.rm) - quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)}
ci.high <- function(x,na.rm=T) {
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) - mean(x,na.rm=na.rm)}

data_summary <- df.data.clean |>
  group_by(predicate_type, answer_type) |>
  summarise(mean_rating = mean(rating),
            ci_low = ci.low(rating),
            ci_high = ci.high(rating)) |>
  ungroup() |>
  mutate(YMin = mean_rating - ci_low,
         YMax = mean_rating + ci_high,
         predicate_type = fct_relevel(predicate_type, "Bad control", "Cognitive", "Emotive", "Good control"),
         answer_type = fct_relevel(answer_type, "Yes", "No"))

summary_plot <- ggplot(data_summary,
       aes(x = predicate_type,
           y = mean_rating,
           fill = predicate_type)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=YMin,
                    ymax=YMax),
                width=.2) +
  facet_grid(. ~ answer_type) +
  labs(x = "Type",
       y = "Score (1=Compl. unnatural, 7=Compl. natural)",
       fill = "Predicate") +
  # scale_fill_manual(values=cbPalette,
  #                    labels = c("Bad control", "Cognitive", "Emotive", "Good control"),
  #                    guide="none") 
  scale_fill_discrete(guide="none") +
  scale_x_discrete(labels = c("Bad control", "Cognitive", "Emotive", "Good control"))
# summary_plot
# ggsave(summary_plot, file="graphs/replication_results.png", width=6, height=4)
```


```{r main_plot, echo=FALSE,out.width="49%", out.height="20%",fig.cap="Plots in the original study (left) and the current study (right). The error bars represent the 95% confidence intervals.",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("graphs/original_results.png","graphs/replication_results.png"))
``` 
Overall, the results in the graphs look similar.

### Confirmatory analysis

The analyses as specified in the analysis plan.  

```{r message=FALSE}
df.data.clean$predicate_type <- as.factor(df.data.clean$predicate_type)
df.data.clean$predicate_type <- relevel(df.data.clean$predicate_type, ref = "Cognitive")
df.data.clean$answer_type <- as.factor(df.data.clean$answer_type)
df.data.clean$answer_type <- relevel(df.data.clean$answer_type, ref = "Yes")

# failed to converge with (predicate_type * answer_type | item), so the by-item random slope is excluded
simple_model = lmer(rating ~ predicate_type * answer_type + (predicate_type * answer_type | participant) + (predicate_type + answer_type | item),
             data = df.data.clean |>
               filter(stimulus_type == "C"))

summary(simple_model)
joint_tests(simple_model)
```



### Exploratory analyses
```{r message=FALSE, warning=FALSE}
# exploratory: compare think (non-factive) and the two types of factives, not collapsing over conditions 
data_summary_all <- df.data.clean |>
  group_by(all_types, answer_type) |>
  summarise(mean_rating = mean(rating),
            ci_low = ci.low(rating),
            ci_high = ci.high(rating)) |>
  ungroup() |>
  mutate(YMin = mean_rating - ci_low,
         YMax = mean_rating + ci_high,
         answer_type = fct_relevel(answer_type, "Yes", "No"))

summary_think_plot <- ggplot(data_summary_all |>
                                 filter(!all_types %in% c("Conj Bad", "Conj Good")),
       aes(x = all_types,
           y = mean_rating,
           fill = all_types)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=YMin,
                    ymax=YMax),
                width=.2) +
  facet_grid(. ~ answer_type) +
  labs(x = "Type",
       y = "Score (1=Compl. unnatural, 7=Compl. natural)",
       fill = "Predicate") +
  scale_fill_manual(values=cbPalette,
                     guide="none")
# summary_think_plot
# ggsave(summary_think_plot, file="graphs/replication_think.png", width=7, height=4)
``` 

```{r exploratory_plot, fig.align="center", echo=FALSE,out.width="80%", out.height="20%",fig.cap="Acceptability results of cognitives, emotives, and non-factive control (think). The error bars represent the 95% confidence intervals.",fig.show='hold',fig.align='center'}
knitr::include_graphics("graphs/replication_think.png")
``` 

```{r message=FALSE, warning=FALSE}
df.data.clean$all_types <- as.factor(df.data.clean$all_types)
df.data.clean$all_types <- relevel(df.data.clean$all_types, ref = "Cognitive")
df.data.clean$answer_type <- relevel(df.data.clean$answer_type, ref = "Yes")


exploratory_model = lmer(rating ~ all_types * answer_type + (all_types * answer_type | participant) + (all_types * answer_type | item),
             data = df.data.clean |>
               # filter(!all_types %in% c("Conj Good", "Conj Bad", "Think Bad")))
                filter(all_types %in% c("Cognitive", "Think Good")))

summary(exploratory_model)
joint_tests(exploratory_model)
```

## Discussion

### Summary of Replication Attempt

<!-- Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.   -->

A linear mixed effects model was used to predict acceptability rating from a fixed effect of predicate type (reference level: `Cognitive`) and answer type (reference level: `Yes`) with the maximal effects structure^[The model failed to converge when the interaction was included as the by-item random slope. The model converged when only the random slopes of main effects `predicate_type` and `answer_type` were included in the random effect model of items.]. There was a significant interaction effect between predicate type and answer type (t = -3.970, $\beta$ = -0.819, SE = 0.206). In addition, emotive predicates had significantly higher Yes-ratings than cognitive predicates (t = 3.252, $\beta$ = 0.554, SE = 0.170). These results are in line with the findings in the original study.

### Commentary

<!-- Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long. -->
