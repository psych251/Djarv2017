---
title: "Replication of Study 2 by Djärv, Zehr, & Schwarz (2017, Proceedings of Sinn und Bedeutung 21)"
author: "Penny Pan"
date: "`r format(Sys.time(), '%B %d, %Y')`"
output:
  html_document:
    toc: yes
    toc_depth: 3
    toc_float:
      collapsed: false
---

<!-- Replication reports should all use this template to standardize reporting across projects.  These reports will be public supplementary materials that accompany the summary report(s) of the aggregate results. -->

## Introduction

Djärv et al. (2017) shows cognitive and emotive factives differ in the entailment of the presupposed content. Specifically, cognitive factives, such as *realize* and *be aware*, entail both the presupposed content in the embedded clause and the attitude/behavior indicated by the predicate. On the other hand, emotive factives, such as *appreciate* and *be happy*, only entail the attitude/behavior. Given a yes/no-continuation acceptability judgement task, the participant rated the *yes, although...* continuation to be more natural for a question with an emotive factive than for a question that contains a cognitive factive.

(Examples from the original paper)

John was happy that his parents are coming to twon, although it turned out he was in fact mistaken/although it turned out that they had to cancel. (Both felicitous).

?? John discovered that his parents are coming to twon, although it turned out he was in fact mistaken/although it turned out that they had to cancel. (Not acceptable as they lead to contradiction).

[Link to the experiment demo](https://farm.pcibex.net/r/xlqNje/)

[Link to the project repository](https://github.com/psych251/Djarv2017)

[Link to the original paper](https://semanticsarchive.net/Archive/DRjNjViN/DjaervZehrSchwarz.pdf)


## Methods

### Power Analysis

<!-- Original effect size, power analysis for samples to achieve 80%, 90%, 95% power to detect that effect size. Considerations of feasibility for selecting planned sample size. -->

### Planned Sample

<!--Planned sample size and/or termination rule, sampling frame, known demographics if any, preselection rules if any. -->
Plannded sample size (2.5 times the original sample size?) All participants are native English speakers.

### Materials

Each stimulus in the experiment consisted of one question and one response. The response was either in the form *yes, although...*, which first provides an affirmation and then denies the content of the embedded content, or in the form *no, because...*, which directly provides a negative response, as shown in (1) and (2), respectively. 


__(1) Yes-response block__

Q: \{Is Maria aware/happy\} that [Mike is moving back to Chicago]?

A: Yes, although he isn't.

__(2) No-response block__

Q: \{Is Maria aware/Is Maria happy\} that [Mike is moving back to Chicago]?

A: No, because he isn't.


In the original study, 24 items were used as the embedded content, and each item was paired with both types of predicates (cognitive and emotive) and both types of responses (*yes, although...* or *no, because...*), resulted in four variations of each item. In addition, the authors considered two emotive predicates, *appreciate* and *be happy*, and two cognitive predicates, *realize* and *be aware*, so that for each type of predicate, one is verbal and the other is adjective, in order to control for the syntactic category.

In addition, 48 filler items were also included to elicit the baseline rating for the yes/no responses. Non-factive predicates, such as *think*, were used the baseline for *yes*, where yes-continuation would acceptable but not the no-continuation (Example 3). On the other hand, questions with two conjunctions were used as baseline for *no*, where the only felicitous response would be the no-continuation (Example 4). 

All items were divided into two blocks based on the answer type. Both the order of the blocks and the order of sentence pairs within each block were randomized.

__(3) *yes*-response baseline/ceiling__

Q. Does Sue think that Bill’s parents are going to the wedding?

A1. \#No, because they are. (infelicitous, "Bad control")

A2. Yes, although they aren’t. (acceptable, "Good control")


__(4) *no*-response baseline/ceiling__

Q. Is John going to Paris and Rome this summer?

A1. No, he's not. (acceptable, "Good control")

A2. \#Yes, although he isn't going to Rome. (infelicitous, "Bad control")

### Procedure	

Each participant saw 72 pairs of question and answer, including 24 critical pairs and 48 filler pairs, and was asked to rate the naturalness of each answer based on a Likert scale, with 1 being "completely unnatural" to 7 being "completely natural." 

### Analysis Plan

<!--Can also quote directly, though it is less often spelled out effectively for an analysis strategy section.  The key is to report an analysis strategy that is as close to the original - data cleaning rules, data exclusion rules, covariates, etc. - as possible. -->

The authors used linear mixed effects regression models to analyze the ratings of the critical pairs. Specifically, they included the predicate type and the answer type as fixed effects. Then, the considered three models, one with the additional main effect of syntactic category, one with the effect of block order, and one simple model with neither of the two. The complex model with all four factors did not converge, and thus was not used. They "tested for the maximally complex models, including all possible interactions of predictors and all random slopes for participants and items as random effects, and (their) different baselines exhausted the logical space of effects and interactions" (p.13).

The simple model, which only includes the main effects of answer type and predicate type and the interaction effect, is the key model. According to the analyses in the original paper, adding the two additional factors did not significantly change the results. This is as predicted, since neither the block order nor the syntactic category of the predicate should affect the entailment of the embedded content.

```{r message=FALSE, warning=FALSE}
# power analysis: simulation to estimate the sample size
####Load Relevant Libraries and Functions
library("simr")
library("knitr") # for knitting things
library("readr")
library("lme4")
library("emmeans")
library("dplyr")
library("tidyverse") # for all things tidyverse

subject <- factor(1:62)
items <- paste0("item", c(1:24))
predicate <- c("emotive", "cognitive")
answer <- c("yes", "no")

subj_full <- rep(subject, 24)
items_full <- rep(items, 62)
answer_full <- rep(rep(answer, each=12),62)
predicate_full <- rep(rep(rep(predicate, each=6),2),62)

covars <- data.frame(id = subj_full, 
                     answer_type = answer_full, 
                     predicate_type = predicate_full,
                     items = items_full)

# sanity check to make sure the numbers are correct
check_covars <- covars |>
  group_by(id, predicate_type, answer_type) |>
  summarise(count  = n())
check_covars

fixed <- c(0.8605) # not sure about what the slopes and slopes are, besides the one for interaction?
rand <- list(0.5, 0.1) # similar issue here for the random effects
res <- 1

# model <- makeLmer(rating ~ answer_type : predicate_type +
#                     (answer_type : predicate_type | items) +
#                     (answer_type : predicate_type | id),
#                   fixef = fixed,
#                   VarCorr = rand,
#                   sigma = res,
#                   data = covars)
# 
# # alternative model that only includes main effects?
# sim <- powerSim(model, nsim=100, test = fcompare(y~answer_type + predicate_type)) 

```


In addition, I also plan to use Bayesian models to analyze the data. 
<!-- You can also pre-specify additional analyses you plan to do.-->

### Differences from Original Study

<!--Explicitly describe known differences in sample, setting, procedure, and analysis plan from original study.  The goal, of course, is to minimize those differences, but differences will inevitably occur.  Also, note whether such differences are anticipated to make a difference based on claims in the original article or subsequent published research on the conditions for obtaining the effect. -->

### Methods Addendum (Post Data Collection)

You can comment this section out prior to final report with data collection.

#### Actual Sample
  Sample size, demographics, data exclusions based on rules spelled out in analysis plan

#### Differences from pre-data collection methods plan
  Any differences from what was described as the original plan, or “none”.


## Results


### Data preparation

Data preparation following the analysis plan.
	
```{r message=FALSE, warning=FALSE}
###Data Preparation

####Import data
# ignore the first 17 lines as they are explanations of the content in each column
df.data <- read.csv("../data/pilot/results.csv", header=FALSE, skip=17)
# add column names
# reception_time -> participant, latin_square -> stimulus_type (either control or filler)
colnames(df.data) <- c("participant", "ip", "controller", "order", "inner_element", "label", "stimulus_type", "legend","answer","correct","rt")


#### Data exclusion / filtering

# ``order'' is also not needed, will extract the item from ``legend''
df.data.tidy <- df.data |>
  select("participant", "legend", "label", "stimulus_type", "answer", "rt") |>
  drop_na(rt) |> # when rt is NA, for the instruction page
  # group_by(reception_time) |> # group by participants id
  # head(-4) |> # remove the last four rows of comments.  alternatively use slice(1:218), but have problem with negative/positive indices?
  # ungroup() |>
  filter(label!="practice") |>
  group_by(legend) |> # group by trial
  mutate(total_rt = sum(rt)) |>
  slice(1) |>
  ungroup() |>
  select("participant", "legend", "stimulus_type", "answer", "label")


#### Prepare data for analysis - create columns etc.
# extract the item name 
df.data.clean <- df.data.tidy |>
  mutate(item = gsub(".*<b>Item</b>_(.*)\\+<b>Group.*", "\\1", legend)) |>
  select(-legend)

# separate by different conditions
df.data.clean <- df.data.clean |>
  mutate(label = gsub('([[:upper:]])', ' \\1', label)) |>
  mutate(exp = str_split_fixed(label, ' ', 5)[,2],
         predicate_type = str_split_fixed(label, ' ', 5)[,3],
         syntactic_category = str_split_fixed(label, ' ', 5)[,4],
         answer_type = str_split_fixed(label, ' ', 5)[,5]) |>
    mutate(predicate_type = recode(predicate_type,
                                  Conj = "Conjunct", 
                                  Cog = "Cognitive",
                                  Emo = "Emotive",
                                  Think = "Think")) |>
  select(-label)

# change the answers to numerical values
df.data.clean <- df.data.clean |>
  mutate(rating = case_when(
    answer == "CompUnnatural" ~ 1,
    answer == "Unnatural" ~ 2,
    answer == "NotNatural" ~ 3,
    answer == "Average" ~ 4,
    answer == "Natural" ~ 5,
    answer == "QuiteNatural" ~ 6,
    answer == "CompNatural" ~ 7
  )) |> 
  select(-answer)

# check the number of each item
# df.test <- df.data.clean |>
#   filter(stimulus_type == "C") |>
#   group_by(item,predicate_type, answer_type) |>
#   summarize(num = n())
```

```{r message=FALSE, warning=FALSE}
# plot the result
# set the theme
theme_set(theme_bw())
# color-blind-friendly palette
cbPalette <- c("#E69F00", "#56B4E9", "#009E73", "#F0E442", "#0072B2", "#D55E00", "#CC79A7") 

## helper functions for the graph
library(bootstrap)
theta <- function(x,xdata,na.rm=T) {mean(xdata[x],na.rm=na.rm)}
ci.low <- function(x,na.rm=T) {
  mean(x,na.rm=na.rm) - quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.025,na.rm=na.rm)}
ci.high <- function(x,na.rm=T) {
  quantile(bootstrap(1:length(x),1000,theta,x,na.rm=na.rm)$thetastar,.975,na.rm=na.rm) - mean(x,na.rm=na.rm)}

data_summary <- df.data.clean |>
  group_by(predicate_type, answer_type) |>
  summarise(mean_rating = mean(rating),
            ci_low = ci.low(rating),
            ci_high = ci.high(rating)) |>
  ungroup() |>
  mutate(YMin = mean_rating - ci_low,
         YMax = mean_rating + ci_high,
         predicate_type = case_when(predicate_type == "Conjunct" ~ "Bad control",
                                    predicate_type == "Think" ~ "Good control",
                                    TRUE ~ predicate_type),
         predicate_type = fct_relevel(predicate_type, "Bad control", "Cognitive", "Emotive", "Good control"),
         answer_type = fct_relevel(answer_type, "Yes", "No"))

summary_plot <- ggplot(data_summary,
       aes(x = predicate_type,
           y = mean_rating,
           fill = predicate_type)) +
  geom_bar(stat="identity") +
  geom_errorbar(aes(ymin=YMin,
                    ymax=YMax),
                width=.2) +
  facet_grid(. ~ answer_type) +
  labs(x = "Type",
       y = "Score (0=Compl. unnatural, 6=Compl. natural)",
       fill = "Predicate") +
  scale_fill_manual(values=cbPalette,
                     labels = c("Bad control", "Cognitive", "Emotive", "Good control"),
                     guide="none")
# summary_plot
ggsave(summary_plot, file="graphs/replication_results.png", width=6, height=4)
```

```{r, echo=FALSE,out.width="49%", out.height="20%",fig.cap="Plots in the original study (left) and the current study (right) ",fig.show='hold',fig.align='center'}
knitr::include_graphics(c("graphs/original_results.png","graphs/replication_results.png"))
``` 

### Confirmatory analysis

The analyses as specified in the analysis plan.  

```{r message=FALSE, warning=FALSE}
df.data.clean$predicate_type <- as.factor(df.data.clean$predicate_type)
df.data.clean$predicate_type <- relevel(df.data.clean$predicate_type, ref = "Cognitive")
df.data.clean$answer_type <- as.factor(df.data.clean$answer_type)
df.data.clean$answer_type <- relevel(df.data.clean$answer_type, ref = "Yes")

# pilot data: grouping factors doesn't have more than 1 sampled level
# simple_model = lmer(rating ~ predicate_type * answer_type + (predicate_type * answer_type  | participant) + (predicate_type * answer_type  | item),
#              data = df.data.clean |>
#                filter(stimulus_type == "C"))
# 
# summary(simple_model)
# comp_model_answer = lmer(rating ~ predicate_type * answer_type * answer_type + (predicate_type * answer_type * answer_type  | participant),
#                          data = df.data.clean |>
#                            filter(stimulus_type == "C"))
```




### Exploratory analyses

Any follow-up analyses desired (not required).  

## Discussion

### Summary of Replication Attempt

Open the discussion section with a paragraph summarizing the primary result from the confirmatory analysis and the assessment of whether it replicated, partially replicated, or failed to replicate the original result.  

### Commentary

Add open-ended commentary (if any) reflecting (a) insights from follow-up exploratory analysis, (b) assessment of the meaning of the replication (or not) - e.g., for a failure to replicate, are the differences between original and present study ones that definitely, plausibly, or are unlikely to have been moderators of the result, and (c) discussion of any objections or challenges raised by the current and original authors about the replication attempt.  None of these need to be long.
